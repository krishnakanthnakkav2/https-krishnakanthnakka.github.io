<!DOCTYPE HTML>
<html lang="en"><head>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-125946091-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-125946091-1');
</script>


  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Krishna Kanth Nakka</title>

  <meta name="author" content="Krishna Kanth Nakka">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>


<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Krishna Kanth Nakka</name>
              </p>
              <p>



                I graduated with a PhD in Computer Science in August 2022 from the Computer Vision Lab at EPFL.
   I was supervised by <a href="https://people.epfl.ch/cgi-bin/people?id=119864&op=bio&lang=en&cvlang=en">Dr. Mathieu Salzmann</a> and <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Prof. Pascal Fua</a> in the areas of computer vision and deep learning.</p>

<p>Following the completion of my Ph.D., I worked as a postdoctoral scientist at the Visual Intelligence for Transportation Lab (VITA) at EPFL under the supervision of <a href="https://people.epfl.ch/alexandre.alahi?lang=en">Prof. Alexandre Alahi</a> until April 2023.</p>

<p>Before joining EPFL in 2017, I spent two years at <a href="https://research.samsung.com/sri-b">Samsung Research Bangalore</a> working on mobile camera algorithms. Prior to that, I graduated from the Department of Electrical Engineering at <a href="http://www.iitkgp.ac.in/">IIT Kharagpur</a> in 2015 with a Dual degree (Masters and Bachelors).</p>

<p>During my undergraduate years, I interned at the <a href="https://www.ualberta.ca/computing-science/index.html">University of Alberta</a>, <a href="https://www.google.com/search?q=cai+uq&oq=CAI+uq&aqs=chrome.0.0i512j0i22i30j0i15i22i30j0i22i30l3j0i390l4.2150j0j4&sourceid=chrome&ie=UTF-8University">the University of Queensland</a>, and <a href="https://www.philips.com/a-w/about/innovation/innovation-hubs/bangalore.html">Philips Research</a>.

 <p>I'm currently working in the Privacy Team, Trustworthy Technology Lab at Huawei Munich Research Center focusing on the privacy of Large Language Models and Auto ML techniques for Differentially Private Federated Learning algorithms.</p>

</p>

            </p>
            <p>
            
              </p>
              <p style="text-align:center">
                <a href="mailto:krishkanth.92@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/Resume_KrishnaKanthNakka.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=g_21RKoAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/krishnakanthnakka/">Github</a>&nbsp/&nbsp
                <a href="https://www.linkedin.com/in/krishna-kanth-nakka-b73a4b44/">LinkedIn</a>&nbsp/&nbsp
                <a href="data/Thesis_final_compressed.pdf">Thesis</a> &nbsp/&nbsp
                <a href="data/Public_Defense_pdf_1.pdf">Thesis Slides</a>


              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/bio3.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/bio3.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
<p>My overarching goal is to develop machine learning models that are both robust and privacy-aware, in the domains of safety and security-critical applications. In my current research, I concentrate on enhancing privacy aspects within Large Language Models (LLMs) and within the context of Federated Learning. Additionally, I explore AutoML techniques to optimize hyperparameters in the Federated Learning setting.</p>

<p>During my doctoral studies, my primary focus was on comprehending the limitations of deep neural networks concerning out-of-distribution and adversarial scenarios, with the aim of improving robustness against adversarial domain shifts. My research encompassed areas such as interpretable models, transfer-based black-box attacks, attack detection, adversarial defenses, anomaly detection, and the evaluation of disentangled representations. While at VITA, my research delved into various projects related to human-pose estimation, tracking, and re-identification, with a particular application in the field of team sports analytics.</p>

<p>I am enthusiastic about collaborating with motivated students and researchers who share an interest in Adversarial Machine Learning. If my research background aligns with your interests, please feel free to reach out to me via email.</p>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vita.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <papertitle>  A unified framework for keypoint-based multi-person pose detection, tracking and re-identification for team sport analysis </papertitle>
              </a>
               <br>
              <strong>Krishna Kanth Nakka</strong>
              <p>Innosuisse VITA-Dartfish, <a href="https://www.epfl.ch/research/domains/transportation-center/research-overview/others/team-sports-analysis-via-image-based-tracking/">Sep 2022 - April 2023 </a> </p>
              <br>
              <p>The objective of this project is to enhance sports player tracking through a unified framework. This involves detecting and tracking semantic keypoints and utilizing re-identification techniques to enhance long-term tracking, especially when players go out of view.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/disentanglement.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
                                          <a href="data/posepaper.pdf">

                <papertitle>Understanding Pose and Appearance Disentanglement in 3D Human Pose Estimation</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Preprint</em>, 2022
               <br>
              <a href="data/posepaper.pdf">Paper</a>
              <br>
              <p>Our analyses show that disentanglement in the three state-of-the-art disentangled representation learning frameworks is far from complete,
              and that their pose codes contain significant appearance information</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pami.jpg" alt="teaser" width="240" height="120">
            </td>
            <td width="75%" valign="middle">
                            <a href="data/pami.pdf">

                <papertitle>Universal, Transferable Adversarial Attacks for Visual Object Trackers</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <a href="data/pami.pdf">Paper</a>
              <br>
              <em>Adversarial Robustness Workshop, European Conference on Computer Vision (ECCV)</em>, 2022
              <p>We propose to learn to generate a single perturbation from the
              object template only, that can be added to every search image and still successfully fool the tracker for the entire video. As a
              consequence, the resulting generator outputs perturbations that are quasi-independent of the template, thereby making them universal
              perturbations.</p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/neurips.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">
                <papertitle>Learning Transferable Adversarial Perturbations</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Neural Information and Processing Systems (NeurIPS)</em>, 2021
              <br>
              <a href="https://proceedings.neurips.cc/paper/2021/hash/7486cef2522ee03547cfb970a404a874-Abstract.html">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Transferable_Perturbations">code</a>
              <br>
              <p>We show that generators trained with mid-level feature separation loss transfers significantly better in cross-model, cross-domain and cross-task setting</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/accv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/ACCV2020/html/Nakka_Towards_Robust_Fine-grained_Recognition_by_Maximal_Separation_of_Discriminative_Features_ACCV_2020_paper.html">
                <papertitle>Towards Robust Fine-grained Recognition by Maximal Separation of Discriminative Features</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> Asian Conference on Computer Vision (ACCV)</em>, 2020
              <br>
               <a href="https://arxiv.org/abs/2006.06028">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/RobustFineGrained/">code</a> /
              <a href="data/slides_accv.pdf">Slides</a>
              <br>
              <p>We improve the robustness by introducing an attention-based regularization mechanism that maximally separates the latent features of discriminative regions of different classes
              while minimizing the contribution of the non-discriminative regions to the final class prediction.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2020.jpg" alt="teaser" width="240" height="170">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">
                <papertitle>Indirect Local Attacks for Context-aware Semantic Segmentation Networks</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong>, <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>European Conference on Computer Vision  (ECCV)</em>, 2020 <strong>[Spotlight]</strong>
              <br>
               <a href="https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123500596.pdf">arXiv</a> /
              <a href="https://github.com/krishnakanthnakka/Indirectlocalattacks/">code</a> /
              <a href="data/slides_eccv.pdf">Slides</a>

              <br>
              <p> We show that the resulting networks are sensitive not only to global attacks, where perturbations affect the entire input image, but also to indirect local attacks
              where perturbations are confined to a small image region that does not overlap with the area that we aim to fool. </p>

            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccv.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1904.07595">
                <papertitle>Detecting the Unexpected via Image Resynthesis</papertitle>
              </a>
              <br>
              <a href="https://adynathos.net/">Krzysztof Lis</a>, <strong>Krishna Kanth Nakka</strong>,  <a href="https://people.epfl.ch/pascal.fua/bio?lang=en">Pascal Fua</a> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em> International Conference on Computer Vision (ICCV) </em>, 2019
              <br>
               <a href="https://arxiv.org/abs/1904.07595">arXiv</a> /
              <a href="https://github.com/cvlab-epfl/detecting-the-unexpected/">code</a> /
              <a href="data/DetectingTheUnexpected_Poster.pdf">Poster</a>

              <br>
              <p> We rely on the intuition that the network will produce spurious labels in regions depicting unexpected anomaly objects.
              Therefore, resynthesizing the image from the resulting semantic map will yield significant appearance differences with respect to the input image which we detect through  an auxiliary network</p>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/iccvw.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1901.02229">
                <papertitle>Interpretable BoW Networks for Adversarial Example Detectio</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>Explainable and Interpretable AI workshop, ICCV</em>, 2018  <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1901.02229">arXiv</a> /
              <a href="data/iccvw_slides.pdf">Slides</a>

              <br>
              <p> We build upon the intuition that, while adversarial samples look very similar to real images, to produce incorrect predictions, they should activate
                codewords with a significantly different visual representation.
              We therefore cast the adversarial example detection problem as that of comparing the input image with the most highly activated visual codeword.</p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/bmvc.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1805.05389">
                <papertitle>Deep Attentional Structured Representation Learning for Visual Recognition</papertitle>
              </a>
              <br>
              <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ">Mathieu Salzmann</a>
              <br>
              <em>British Media Vision Conference (BMVC)</em>, 2018
              <br>
               <a href="https://arxiv.org/abs/1805.05389">arXiv</a> /
                              <a href="data/BMVC2018_Poster.pdf">Poster</a>

                             <br>
              <p> we introduce an attentional structured representation learning framework that incorporates an image-specific attention mechanism within the aggregation process. </p>

            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/eccv2016.jpg" alt="teaser" width="240" height="180">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/1609.07727">
                <papertitle>Deep learning based fence segmentation and removal from an image using a video sequence</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>International Workshop on Video Segmentation, ECCV</em>, 2016 <strong>[Oral]</strong>
              <br>
               <a href="https://arxiv.org/abs/1609.07727">arXiv</a> /
              <a href="data/Slides_ECCV2016.pdf">Slides</a>
              <br>
              <p>  We use  knowledge of spatial locations of fences to subsequently estimate  occlusion-aware optical flow. We then fuse the occluded information from neighbouring frames
              by solving inverse problem of denoising</p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/josaa.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">
                <papertitle>Detection and removal of fence occlusions in an image using a video of the static/dynamic scene</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Journal of the Optical Society of America A (JOSA A) </em>, 2016
              <br>
               <a href="https://opg.optica.org/josaa/viewmedia.cfm?uri=josaa-33-10-1917">arXiv</a> /
              <a href="data/JOSA.pdf">PDF</a>
              <br>
              <p> Our approach of defencing is as follows: (i) detection of spatial locations of fences/occlusions in the frames of the video, (ii) estimation
              of relative motion between the observations, and (iii) data fusion to fill in occluded pixels in the reference image. We assume the de-fenced image as a Markov random
            field and obtain its maximum a posteriori estimate by solving the corresponding inverse problem. </p>

            </td>
          </tr>


           <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/acpr.jpg" alt="teaser" width="240" height="200">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7486506">
                <papertitle>My camera can see through fences: A deep learning approach for image de-fencing</papertitle>
              </a>
              <br>
               SankarGanesh Jonna, <strong>Krishna Kanth Nakka</strong> and <a href="https://scholar.google.com/citations?hl=en&user=n-B0jr4AAAAJ"> Rajiv Ranjan Sahay</a>
              <br>
              <em>Asian Conference on Pattern Recognition (ACPR),  </em>, 2015
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7486506">arXiv</a> /
              <a href="data/My_camera_can_see_through_fences_A_deep_learning_approach_for_image_de-fencing.pdf">PDF</a> /
              <a href="data/acpr_poster.pdf">Poster</a>
               <p> We propose a semi-automated de-fencing algorithm using a video of the dynamic scene. The inverse problem offence removal is solved using split Bregman
                technique assuming total variation of the de-fenced image as the regularization constraint.
              </p>

              <br>

            </td>
          </tr>



          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/globalsip.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/7032076">
                <papertitle>3D-to-2D mapping for user interactive segmentation of human leg muscles from MRI data</papertitle>
              </a>
              <br>
              Nilanjan Ray, Satarupa Mukherjee, <strong>Krishna Kanth Nakka</strong>, Scott T. Acton, Silvia S. Blanker
              <br>
              <em>Signal and Information Processing, GlobalSIP</em>, 2014
              <br>
               <a href="https://ieeexplore.ieee.org/abstract/document/7032076">arXiv</a> /
              <a href="data/3D-to-2D_mapping_for_user_interactive_segmentation_of_human_leg_muscles_from_MRI_data.pdf">PDF</a>
              <br>
              <p>
              We proposing a framework for user interactive segmentation of MRI of human leg muscles built upon the the strategy of bootstrapping with minimal supervision.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/nus1.jpg" alt="teaser" width="240" height="240">
            </td>
            <td width="75%" valign="middle">
              <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">
                <papertitle>Non-uniform sampling in EPR: optimizing data acquisition for Hyscore spectroscopy</papertitle>
              </a>
              <br>
               <strong>Krishna Kanth Nakka</strong> Y. A. Tesiram, I. M. Brereton,  M. Mobli and J. R. Harmer
              <br>
              <em>Physical Chemistry Chemical Physics (PCCP)</em>, 2014
              <br>
               <a href="https://pubs.rsc.org/en/content/articlepdf/2014/cp/c4cp02172j">Paper</a> /
                <a href="data/pccp_main.pdf">PDF</a> /
                <a href="data/pccp_supp.pdf">Supp</a>
              <br>
              <p>We show through non-linear sampling scheme with maximum entropy reconstruction technique in HYSCORE, the experimental times can be shortened by
              approximately an order of magnitude as compared to conventional linear sampling with negligible loss of information
              </p>

            </td>
          </tr>
        </tbody>
      </table>

        

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
  <tbody>
    <tr>
      <td style="padding:20px;width:100%;vertical-align:middle">
        <heading>Scholarships</heading>
        <p>I'm deeply grateful for the generous scholarships I received throughout my academic journey. Some of these scholarships include:</p>
        <ul>

          <li><a href="https://www.mitacs.ca/en/programs/globalink/globalink-research-internship">MITACS Summer Research Scholarship</a> to conduct research at the University of Alberta</li>
        <li><a href="https://science.uq.edu.au/student-support/scholarships/undergraduate-scholarships/uq-summer-research-program">University of Queensland Summer Research Scholarship</a> to support the internship at <a href="https://cai.centre.uq.edu.au/">Center for Advanced Imaging Institute</a></li>
        <li><a href="https://www.epfl.ch/education/phd/edic-computer-and-communication-sciences/edic-for-phd-students/">EDIC PhD Fellowship</a> for pursuing the first year of doctoral studies at EPFL</li>
        </ul>


      </td>
    </tr>
  </tbody>
</table>

  
	

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Reviewer</heading>

              <p>I have peer-reviewed more than 50 articles. Some of them include:</p>
<ul>
  <li>Reviewer at Transactions on Pattern Analysis and Machine Intelligence, 2019, 2023</li>
  <li>Reviewer at Neural Information Processing Systems, NeurIPS 2021, 2022, 2023</li>
  <li>Reviewer at Computer Vision and Pattern Recognition, CVPR 2023</li>
  <li>Reviewer at International Conference on Computer Vision and Pattern Recognition, ICCV 2023</li>
  <li>Reviewer at International Conference on Machine Learning, ICML 2023</li>
  <li>PC Member and Reviewer at New Frontiers in Machine Learning, International Conference on Machine Learning, ICML 2023</li>
  <li>Reviewer at British Machine Vision Conference, BMVC 2023</li>
  <li>Reviewer at Winter Application for Computer Vision Conference, WACV 2019, 2024</li>
  <li>Reviewer at International Conference on Artificial Intelligence and Statistics (AISTATS) 2024</li>
  <li>Reviewer at International Conference on Learning Representations, ICLR 2024</li>
</ul>


            
            </td>
          </tr>
        </tbody>
      </table>

 <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Outside research</heading>

<p>Apart from work, I spend time with lakes. Thanks for visiting this page. I leave you with a thought that always lingers in my mind by Sirivennela gaaru:</p>

<blockquote>
  <p>‡∞®‡±Å‡∞µ‡±ç‡∞µ‡±Å ‡∞§‡∞ø‡∞®‡±á ‡∞™‡±ç‡∞∞‡∞§‡∞ø ‡∞í‡∞ï ‡∞Æ‡±Ü‡∞§‡±Å‡∞ï‡±Å ‡∞à ‡∞∏‡∞Ç‡∞ò‡∞Ç ‡∞™‡∞Ç‡∞°‡∞ø‡∞Ç‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø, ‡∞ó‡∞∞‡±ç‡∞µ‡∞ø‡∞Ç‡∞ö‡±á ‡∞à ‡∞®‡±Ä ‡∞¨‡±ç‡∞∞‡∞§‡±Å‡∞ï‡±Å ‡∞à ‡∞∏‡∞Æ‡∞æ‡∞ú‡∞Æ‡±á ‡∞Æ‡∞≤‡∞ø‡∞ö‡∞ø‡∞Ç‡∞¶‡∞ø ...<br>
  ‡∞ã‡∞£‡∞Ç ‡∞§‡±Ä‡∞∞‡±ç‡∞ö‡±Å ‡∞§‡∞∞‡±Å‡∞£‡∞Ç ‡∞µ‡∞∏‡±ç‡∞§‡±á ‡∞§‡∞™‡±ç‡∞™‡∞ø‡∞Ç‡∞ö‡±Å‡∞ï‡±Å ‡∞™‡±ã‡∞§‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞µ‡∞æ, ‡∞§‡±Ü‡∞™‡±ç‡∞™ ‡∞§‡∞ó‡∞≤‡∞™‡±Ü‡∞ü‡±ç‡∞ü‡±á‡∞∏‡±ç‡∞§‡∞æ‡∞µ‡∞æ ‡∞è‡∞∞‡±Å ‡∞¶‡∞æ‡∞ü‡∞ó‡∞æ‡∞®‡±á ...</p>
</blockquote>

<blockquote>
  <p>Loosely translates to:</p>
</blockquote>

<blockquote>
  <p>Every single grain you eat is made by this society, And this life of which you are so proud of, is shaped by the society, And when the time comes for a payback, Are you escaping away? Will you burn the boat once you cross the stream?</p>
</blockquote>

<blockquote>
  <p>Quoting Audrey, "Nothing is more important than empathy for another human being‚Äôs suffering. Nothing‚Äînot career, not wealth, not intelligence, certainly not status. We have to feel for one another if we‚Äôre going to survive with dignity."</p>
</blockquote>



  </td>
          </tr>
        </tbody>
      </table>



  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px">
              <br>
              <p style="text-align:right;font-size:small;">
                Credits: Webpage template from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
              </p>
            </td>
          </tr>
        </tbody></table>


      </td>
    </tr>
  </table>






</body>
</html>
